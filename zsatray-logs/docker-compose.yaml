# Based on https://www.elastic.co/guide/en/elasticsearch/reference/8.17/docker.html#docker-compose-file
#          https://github.com/elastic/elasticsearch/blob/8.17/docs/reference/setup/install/docker/docker-compose.yml

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.3
    volumes:
      - elasticsearch-config:/usr/share/elasticsearch/config
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    environment:
      node.name: elasticsearch
      cluster.name: cluster
      cluster.initial_master_nodes: elasticsearch
      ELASTIC_PASSWORD: &elastic-password elastic
      bootstrap.memory_lock: true
      xpack.security.enabled: true
      xpack.security.http.ssl.enabled: true
      xpack.security.http.ssl.key: certs/elasticsearch/elasticsearch.key
      xpack.security.http.ssl.certificate: certs/elasticsearch/elasticsearch.crt
      xpack.security.http.ssl.certificate_authorities: certs/ca/ca.crt
      xpack.security.transport.ssl.enabled: true
      xpack.security.transport.ssl.key: certs/elasticsearch/elasticsearch.key
      xpack.security.transport.ssl.certificate: certs/elasticsearch/elasticsearch.crt
      xpack.security.transport.ssl.certificate_authorities: certs/ca/ca.crt
      xpack.security.transport.ssl.verification_mode: certificate
      xpack.license.self_generated.type: basic
      xpack.ml.use_auto_machine_memory_percent: true
    command: >
      bash -c '
        cd /usr/share/elasticsearch;
        mkdir -p config/certs;

        echo "Creating CA";
        bin/elasticsearch-certutil ca --pem -out config/certs/ca.zip;
        unzip config/certs/ca.zip -d config/certs;

        echo "Creating certs";
        echo -ne \
          "instances:\n" \
          "  - name: elasticsearch\n" \
          "    dns:\n" \
          "      - elasticsearch\n" \
          "      - localhost\n" \
          "    ip:\n" \
          "      - 127.0.0.1\n" \
          > config/certs/instances.yml;
        bin/elasticsearch-certutil cert --pem -out config/certs/certs.zip \
          --in config/certs/instances.yml \
          --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
        unzip config/certs/certs.zip -d config/certs;

        /bin/tini -s -- /usr/local/bin/docker-entrypoint.sh;
      '
    healthcheck:
      test: >
        bash -c '
          cacert=/usr/share/elasticsearch/config/certs/ca/ca.crt;
          [ -f $$cacert ] && \
          [ $$(curl --cacert $$cacert -u elastic:$$ELASTIC_PASSWORD -s -o/dev/null -w "%{response_code}\n" https://localhost:9200) -eq 200 ] && \
          [ $$(curl --cacert $$cacert -u elastic:$$ELASTIC_PASSWORD -s "https://localhost:9200/_cat/health?h=status") = green ];
        '
    # TODO Consider adding a mem_limit
    ulimits:
      memlock:
        soft: -1
        hard: -1
  kibana:
    depends_on:
      elasticsearch:
        condition: service_healthy
    image: docker.elastic.co/kibana/kibana:8.17.3
    volumes:
      - elasticsearch-config:/usr/share/elasticsearch/config:ro
      - kibana-data:/usr/share/kibana/data
    ports:
      - 5601:5601
    environment:
      elastic_password: *elastic-password
      KIBANA_PASSWORD: &kibana-password kibana
      SERVERNAME: kibana
      ELASTICSEARCH_HOSTS: https://elasticsearch:9200
      ELASTICSEARCH_USERNAME: kibana_system
      ELASTICSEARCH_PASSWORD: *kibana-password
      ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES: /usr/share/kibana/config/certs/ca/ca.crt
    command: >
      bash -c '
        cd /usr/share/kibana;

        cp -r /usr/share/elasticsearch/config/certs config/;

        curl --cacert config/certs/ca/ca.crt -u elastic:$$elastic_password -H "Content-Type: application/json" https://elasticsearch:9200/_security/user/kibana_system/_password -d "{\"password\":\"$$KIBANA_PASSWORD\"}";

        /bin/tini -s -- /usr/local/bin/kibana-docker
      '
    # TODO Consider adding a mem_limit
  logstash:
    depends_on:
      elasticsearch:
        condition: service_healthy
    image: docker.elastic.co/logstash/logstash:8.17.3
    volumes:
      - type: volume
        source: elasticsearch-config
        target: /usr/share/elasticsearch/config
        read_only: true
      - type: bind
        source: logs
        target: /tmp/zsatray-logs
        read_only: true
    configs:
      - source: logstash.conf
        target: /usr/share/logstash/config/logstash.conf
    user: '0'
    command: >
      bash -c '
        cd /usr/share/logstash;

        # FIXME STOPPED Ideally simply launch logstash here but there seems to be a bit of munging still needed
        #               Apparently there is binary data in some of the log entries, these will need to be base64 encoded first
        #               Example: awk 'NR==62492' logs/ZSATray_2025-01-29-20-21-11.099011.log
        #               Write a Python? script to handle this and write a separate log file for logstash ingest?
        apt update && apt install -y less vim;

        sleep inf;
        #bin/logstash -f config/logstash.conf;
      '
volumes:
  elasticsearch-config:
  elasticsearch-data:
  kibana-data:
configs:
  logstash.conf:
    content: |
      input {
        file {
          mode => read
          path => [ "/tmp/zsatray-logs/*.log" ]

          # Ref: https://www.elastic.co/docs/reference/logstash/plugins/plugins-codecs-multiline
          codec => multiline {
            pattern => "\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}"
            negate => true
            what => "previous"
          }
        }
      }

      filter {
        if [message] =~ "^Redirected stderr$" { drop {} }

        # Refs: https://www.elastic.co/docs/reference/logstash/plugins/plugins-filters-grok
        #       https://github.com/logstash-plugins/logstash-patterns-core/blob/main/patterns/ecs-v1/grok-patterns
        grok {
          pattern_definitions => {
           "TIMESTAMP" => "\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d{3}"
           "TIMEZONE" => "[-+]?\d{4}"
           "SYSTEM" => "[^]]+"
          }
          match => { "message" => "^%{TIMESTAMP:timestamp}\d{3}\(%{TIMEZONE:timezone}\)\[\d+:\d+\]\s+%{WORD:loglevel}(?:\s+\[%{SYSTEM:system}\](?:\s+\[%{SYSTEM:subsystem}\])?)?\s+%{GREEDYDATA:logentry}" }
        }

        # Ref: https://www.elastic.co/docs/reference/logstash/plugins/plugins-filters-date
        date {
          match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
          timezone => "%{timezone}"
        }

        mutate {
          rename => { "logentry" => "message" }
          remove_field => [ "timestamp", "timezone" ]
        }
      }

      output {
        elasticsearch {
          hosts => [ "https://elasticsearch:9200" ]
          index => "zsatray"
          user => "elastic"
          # TODO Work out how to use the elastic-password alias here
          password => "elastic"
          ssl_certificate_authorities => [ "/usr/share/elasticsearch/config/certs/ca/ca.crt" ]
        }
      }
